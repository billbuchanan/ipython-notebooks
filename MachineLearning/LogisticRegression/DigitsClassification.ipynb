{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digits classification using logistic regression\n",
    "\n",
    "## AUTHOR: Rajesh Bondugula\n",
    "\n",
    "**This notebook implements classification of MNIST digits using one-vs-all logistic regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import numpy and matplotlib modiules.\n",
    "import scipy.special as scp_spl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from loader import loadmnist\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "css =\"\"\"\n",
    "body {\n",
    "    margin: 0;\n",
    "    font-family: Helvetica;\n",
    "}\n",
    "table.dataframe {\n",
    "    border-collapse: collapse;\n",
    "    border: none;\n",
    "}\n",
    "table.dataframe tr {\n",
    "    border: none;\n",
    "}\n",
    "table.dataframe td, table.dataframe th {\n",
    "    margin: 0;\n",
    "    border: 1px solid white;\n",
    "    padding-left: 0.25em;\n",
    "    padding-right: 0.25em;\n",
    "}\n",
    "table.dataframe th:not(:empty) {\n",
    "    background-color: #fec;\n",
    "    text-align: left;\n",
    "    font-weight: normal;\n",
    "}\n",
    "table.dataframe tr:nth-child(2) th:empty {\n",
    "    border-left: none;\n",
    "    border-right: 1px dashed #888;\n",
    "}\n",
    "table.dataframe td {\n",
    "    border: 2px solid #ccf;\n",
    "    background-color: #f4f4ff;\n",
    "}\"\"\"\n",
    "\n",
    "HTML('<style>{}</style>'.format(css))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data set\n",
    "\n",
    "- Download the data set from [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)\n",
    "- Load the data using loadmnist() from loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the MNIST data.\n",
    "X_train, y_train = loadmnist('train-images.idx3-ubyte', 'train-labels.idx1-ubyte')\n",
    "X_test, y_test = loadmnist('t10k-images.idx3-ubyte', 't10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement the cost function\n",
    "\n",
    "$$\n",
    "J\\left(\\theta\\right) = \\frac{1}{m}\\sum_{i=1}^{m} \\left[-y^{(i)}log\\left(h_{\\theta}\\left(x^{(i)}\\right)\\right) -\\left(1-y^{(i)}\\right)log\\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right) \\right] \\\\ x^{\\left(i\\right)} is \\: the \\: i^{th} sample \\: of \\:x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cost(theta, X, y, lamba_reg=0):\n",
    "    \"\"\"The function returns the cost of the logistic regression\n",
    "\n",
    "       X     : Input array mxn. m rows are number of samples and n features for each sample.\n",
    "       y     : Original class of the input\n",
    "       theta : training parameters\n",
    "    \"\"\"\n",
    "    # Get number of rows\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Convert y and theta into vectors.\n",
    "    y = np.array(y).reshape(len(y), 1)\n",
    "    theta = np.array(theta).reshape(len(theta), 1)\n",
    "\n",
    "    # Append ones as first column to input matrix X.\n",
    "    # This is because theta is (n + 1) x 1 due to the intercept.\n",
    "    # The feature value for intercept is 1.\n",
    "    if len(theta) - X.shape[1] == 1:\n",
    "        X = np.column_stack([np.ones(m), X])\n",
    "\n",
    "    # Calculate the h(x)\n",
    "    h = scp_spl.expit(np.dot(X, theta))\n",
    "\n",
    "    # Due the precision constraints, value close to 1 is rounded to 1 and hence log(1-h) is not defined.\n",
    "    # We adjust the presision so that log is valid.\n",
    "    # We pick a small value for which precision is retained. exp(-37) is found by trial and error method.\n",
    "    # We make sure that none of the values neither 0 nor 1.\n",
    "    precisoin_adjust_value = np.exp(-37)\n",
    "\n",
    "    h = np.where(h == 1, 1 - precisoin_adjust_value, h)\n",
    "    h = np.where(h == 0, precisoin_adjust_value, h)\n",
    "\n",
    "    # Calculate the cost\n",
    "    J = sum(-y * np.log(h) - (1 - y) * np.log(1 - h)) / m\n",
    "    \n",
    "    # Adjust the cost to add the regularization term\n",
    "    reg_value = lamba_reg/(2.0 * m) * sum(np.where(np.arange(len(theta)) == 0, 0, theta.flatten()**2))\n",
    "    return J[0] + reg_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement the gradient function\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\: J(\\theta)}{\\partial \\theta _j} = \\frac{1}{m}\\sum_{i = 1}^{m} \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\theta_j := \\theta_j - \\alpha \\frac{\\partial \\: J(\\theta)}{\\partial \\theta _j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    h_{\\theta}\\left(x\\right)=\\frac{1}{1 + e^{-\\theta^{T}x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(theta, X, y, lambda_reg=0):\n",
    "    \"\"\"Calculate the gradient of cost function at theta\"\"\"\n",
    "\n",
    "    # Get number of rows\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Convert y and theta into vectors.\n",
    "    y = np.array(y).reshape(len(y), 1)\n",
    "    theta = np.array(theta).reshape(len(theta), 1)\n",
    "\n",
    "    # Append ones as first column to input matrix X.\n",
    "    # This is because theta is (n + 1) x 1 due to the intercept.\n",
    "    # The feature value for intercept is 1.\n",
    "    if len(theta) - X.shape[1] == 1:\n",
    "        X = np.column_stack([np.ones(m), X])\n",
    "\n",
    "    # Calculate the h(x)\n",
    "    h_y = scp_spl.expit(np.dot(X, theta)) - y\n",
    "    grad = sum(h_y.repeat(X.shape[1], axis=1) * X)/ m\n",
    "    \n",
    "    # Add the regularization term\n",
    "    grad += float(lambda_reg) * np.where(np.arange(len(theta)) == 0, 0, theta.flatten()) / m\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perfom BFGS on training data to find optimum model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform BFGS and find the parameters that minimize the function.\n",
    "def get_model_parameters(X, y, param_size=3, lambda_reg=0):\n",
    "    def f(theta):\n",
    "        \"\"\"\"\"\"\n",
    "        return calculate_cost(theta, X, y, lambda_reg)\n",
    "\n",
    "    def fprime(theta):\n",
    "        return calculate_gradient(theta, X, y, lambda_reg)\n",
    "\n",
    "    # Initial theta assigned to 0\n",
    "    initial_theta = np.zeros(param_size)\n",
    "\n",
    "    return fmin_bfgs(f, initial_theta, fprime, disp=True, maxiter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get the model parameters\n",
    "\n",
    "- We get the model parameters using BFGS\n",
    "- The parameters we get are in one-vs-all fashion.\n",
    "- We calculate the probability of one class (label 1) by treating all other classes are with label 0.\n",
    "- We calculate this for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -32.931025\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -32.390375\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -32.900540\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -32.796340\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -32.958668\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -33.214043\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -32.921717\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -32.700119\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -32.974033\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -32.900094\n",
      "         Iterations: 1\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajeshb\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:34: DeprecationWarning: numpy boolean negative, the `-` operator, is deprecated, use the `~` operator or the logical_not function instead.\n"
     ]
    }
   ],
   "source": [
    "for c in range(10):\n",
    "    if c:\n",
    "        tuned_theta = np.column_stack([tuned_theta, get_model_parameters(X_train, y_train == c, X_train.shape[1] + 1)])\n",
    "    else:\n",
    "        tuned_theta = get_model_parameters(X_train, y_train == c, X_train.shape[1] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classify the digits\n",
    "\n",
    "$$\n",
    "Classify \\: x \\: in \\: class \\: Argmax\\: i \\: in: \\: \\theta _i ^Tx \\\\\n",
    "i \\: is \\: the \\: class \\\\\n",
    "\\theta \\: is \\: the \\: matrix \\: with \\: model \\: parameters \\\\\n",
    "Each \\: column \\: of \\: \\theta \\: represents \\: parameters \\: of \\: class\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "\n",
    "    # Get number of rows\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Convert theta into vectors if it is an array.\n",
    "    if len(theta.shape) == 1:\n",
    "        theta = np.array(theta).reshape(len(theta), 1)\n",
    "\n",
    "    # Append ones as first column to input matrix X.\n",
    "    # This is because theta is (n + 1) x 1 due to the intercept.\n",
    "    # The feature value for intercept is 1.\n",
    "    if len(theta) - X.shape[1] == 1:\n",
    "        X = np.column_stack([np.ones(m), X])\n",
    "    \n",
    "    # return the class which has maximum probability.\n",
    "    return np.dot(X, theta).argmax(axis=1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validate the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction score =  71.475 %\n"
     ]
    }
   ],
   "source": [
    "predicted_y = predict(X_train, tuned_theta)\n",
    "print \"Prediction score = \", sum(predicted_y == y_train)*100/float(len(y_train)), \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validate the model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction score =  72.4 %\n"
     ]
    }
   ],
   "source": [
    "predicted_y = predict(X_test, tuned_theta)\n",
    "print \"Prediction score = \", sum(predicted_y == y_test)*100/float(len(y_test)), \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
